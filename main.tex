\documentclass{beamer}
\mode<presentation> {
	\usetheme{Copenhagen}
	% remove the navigation symbols from the bottom of all slides
	\setbeamertemplate{navigation symbols}{}
}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{float}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{mathtools}
\usepackage{amssymb,amsmath}
\usepackage{listings}
\usepackage{biblatex}
\usepackage{media9, xcolor}
\usepackage{hyperref}
\usepackage{bm}

\addbibresource{bib.bib}

\lstset{columns=fullflexible}
\newcommand{\link}[1]{{\color{blue}\href{#1}{#1}}}
% L-Norm command
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\title{In-vehicle baby alert system}
\subtitle{Advanced Digital Image Processing project}
\author{F. Casciola, E. G. Ceroni, N. Landolfi} %ordine alfabetico
\institute[Unisi]{Università degli Studi di Siena}
\date{date TBD}

%Per poter compilare le slides con TexStudio, verificare che:
%1. Options - Configure TexStudio - Commands - Biber sia uguale a `biber.exe %`
%2. Options - Configure TexStudio - Build - Default Biblio Tool sia uguale a `Biber`

\begin{document}
	
	\frame{\titlepage}
	
	\section{Introduction}
	
	\begin{frame}
		\frametitle{Introduction}
		Vehicular heatstroke is largely underestimated by the general public. The majority of parents are misinformed and likely to believe that they could \textbf{never forget} their child in a vehicle.
		
		\bigskip
		
		In over 55\% of these cases, the person responsible for the child’s death unknowingly left them in the vehicle. The most dangerous mistake one can make is to think leaving a child alone in a vehicle could never happen to them.
	\end{frame}

	\begin{frame}
		\frametitle{Introduction}
		The inside of a vehicle heats up very quickly! Even with the windows cracked, the temperature inside a car can reach \textbf{51 degrees Celsius} in minutes.
		
		\bigskip
				
		A child's  body overheats three to five times faster compared to an adult, and heatstroke occurs when the body's temperature exceeds 40 degrees Celsius and the body organs begin to shut down.
	\end{frame}

	\begin{frame}
		\frametitle{Introduction: Some Data}
		\begin{figure}
			\centering
			\includegraphics[width=0.8\textwidth, height=0.8\textheight]{img/heatstroke_hist.pdf}
			\label{fig:heatstroke_hist}
		\end{figure}		
	\end{frame}

	\begin{frame}
		\frametitle{Introduction: Project Proposal}
		Based on what we have learned about Computer Vision and Image Processing, a possible solution would be to design a new system which enables \textbf{adult/child's face detection}.
		
		Even better, a hybrid solution which combines several way of measuring/sensing the child's presence would be more robust.
		
		\bigskip
		
		Our proposal is composed of three main steps:
		\begin{itemize}
			\item Collecting the data and building a dataset;
			\item Model selection and synthetic testing;
			\item Field testing of the best model.
		\end{itemize}
	\end{frame}
	
	\section{The Dataset}
	
	\begin{frame}
		\frametitle{The Dataset: Collecting The Data}
		This is the most challenging part of the project. Getting pictures of children under the age of 3 years old is not that easy.
		
		\bigskip
	
		In the beginning, we scraped images from Google Images, but we opted for a pre-existing licensed dataset \footfullcite{eidinger2014age}.
	\end{frame}

	\begin{frame}
		\frametitle{The Dataset: Sub-sampling And Dataset Adjustments}
		Since the pre-existing dataset is designed for a multi-class age classification task, we applied sub-sampling.
		
		\bigskip
		
		This yields an equal number of samples for adults and children, thus focusing the problem on a \textbf{binary classification task}.
		
		\bigskip
		
		Moreover, we decided that the images should mostly contain faces with as little background as possible. To this end, we fed our images into a face extractor\footnote{We settled for MTCNN over HAAR cascade.}.

	\end{frame}

	\begin{frame}
		\frametitle{Dataset - Definitive Version}
		Eventually, the dataset has been split in:		
		\begin{itemize}
			\item Training set: 3520 child faces and 3624 adult faces
			\item Validation set: 379 child faces and 401 adult faces
			\item Test set: 387 child faces and 238 adult faces
		\end{itemize}
	\end{frame}

	\section{Proposed Models}
	
	\begin{frame}
		\frametitle{Use-Case Overview}
		The classification task consists of 3 steps:
		\begin{enumerate}
			\item Image acquisition from a USB camera (e.g Logitech C270);
			\item Face extraction with MTCNN;
			\item Classification of the extracted faces.
		\end{enumerate}
	\end{frame}	
	
	\begin{frame}
		\frametitle{Face extractor}
		As mentioned above, we used a face extractor for two reasons:
		\begin{itemize}
			\item Training set creation: labeling faces by hand was too slow and tedious
			\item Extraction of faces from the acquired image (main use case)
		\end{itemize}
		We began with HAAR cascade, both frontal and lateral, then switched to MTCNN, which proved far superior.
	\end{frame}
	
	\begin{frame}
		\frametitle{MTCNN}
		Framework:
		\begin{itemize}
			\item Image resizing for the creation of a piramid of images
			\item The image piramid is fed to three different CNNs:
			\begin{enumerate}
				\item First, the \textbf{P-Net} produces a large number of candidate BBs\footnote{BB = Bounding box} and performs BB regression, followed by NMS\footnote{Non-maximum suppression} for merging the overlapping ones.
				\item Surviving candidates are fed to the \textbf{R-net} that performs BB regression and again NMS.
				\item At last, the survived boxes are fed to the \textbf{Q-net} that performs similarly to the R-net but it is more complicated and outputs the positions of \textbf{five facial landmarks}.
			\end{enumerate}
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\frametitle{MTCNN}
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{img/schema_mtcnn.JPG}
    		\caption{MTCNN schematics\footfullcite{zhang2016joint}.}
    		\label{fig:schema_mtcnn}
		\end{figure}
	\end{frame}
	
	\begin{frame}
		\frametitle{Fisherface: Generalities}
		Given the real-time nature of the problem we are facing, we decided to try and use also a leaner method to determine whether a face in the image belongs to a child or an adult: Fisherface.
		
		\bigskip
		
		Each sample is flattened into a single vector during the training phase. Then, a PCA\footnote{Principal Component Analysis.} is performed over all the dataset, in order to find a new basis to represent our data with a reduced dimensionality\footfullcite{Belhumeur1997}.
		 
	\end{frame}
	
	\begin{frame}
		\frametitle{Fisherface: Generalities}
		Once the dataset is projected onto the new basis, we use it to compute the covariance matrix and the mean vector for each class.
		
		\bigskip
		
		The covariance matrices and the mean vectors are used to find a generalized eigenvector onto which we can project the data achieving a good separation between the projections related to different classes.
		\begin{figure}
			\centering
			\includegraphics[width=0.8\textwidth]{img/Fisherface.png}
    		\caption{\begin{small}
    		Values of the projection onto the generalized eigenvector. The triangles indicate the first standard deviation w.r.t the mean.
    		\end{small}}
    		\label{fig:fisherface_projection}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{Fisherface: Generalities}
		Computing the dot product between the basis resulting from the PCA and the generalized eigenvector, we obtain a transformation which projects the flattened pictures directly on the latter. This makes the method particularly suitable for our problem.
		
		\bigskip
		
		In our use case, the pictures extracted by the MTCNN will only have to go through the following steps:
		\begin{itemize}
		\item The picture must be resized and flattened, then;
		\item Projected onto the generalized eigenvector, and finally;
		\item The projection has to be compared with a threshold $T$ (if it is $>T$ it will be classified as child, otherwise as adult).
		\end{itemize}
	\end{frame}

	\begin{frame}
		\frametitle{Fisherface: The Threshold}
		According to the Fisher's linear discriminant theory, the threshold $T$ that best separates the data is
		
		\begin{equation*}
		T = \frac{\bm{w} \cdot (\bm{\mu}_0 + \bm{\mu}_1)}{2}, 
		\end{equation*}
		
		where $\bm{w}$ is the generalized eigenvector and $\bm{\mu}_0, \bm{\mu}_1$ are the means of the two classes after they've been projected on the principal components' basis.
		
		\bigskip
		 
		The following table shows the results (on the test set) for discriminating adults and children, using different picture (input) sizes and $n \in \left\{ 100,200,300,400,500\right\}$ principal components.
	\end{frame}

	\begin{frame}
		\frametitle{Fisherface: Test table}
		\begin{table}[]
			\centering
			\resizebox{0.76\textwidth}{!}{%
				\begin{tabular}{|l|l|l|l|l|}
					\hline
					\begin{tabular}[c]{@{}l@{}}Input \\ Size\end{tabular} &
					\begin{tabular}[c]{@{}l@{}}Principal \\ Components\end{tabular} &
					\begin{tabular}[c]{@{}l@{}}Accuracy on \\ Children\end{tabular} &
					\begin{tabular}[c]{@{}l@{}}Accuracy on \\ Adults\end{tabular} &
					Threshold \\ \hline
					100x100 & 500 & 88,63 & 72,27 & 1,485 \\ \cline{2-5} 
					& 400 & 88,37 & 71,43 & 1,477 \\ \cline{2-5} 
					& 300 & 89,14 & 70,17 & 1,448 \\ \cline{2-5} 
					& 200 & 89,4  & 70,59 & 1,415 \\ \cline{2-5} 
					& 100 & 88,37 & 68,48 & 1,394 \\ \hline
					50x50 & 500 & 88,11 & 72,27 & 1,484 \\ \cline{2-5} 
					& 400 & 88,89 & 72,27 & 1,467 \\ \cline{2-5} 
					& 300 & 88,89 & 70,17 & 1,450 \\ \cline{2-5} 
					& 200 & 88,63 & 70,17 & 1,408 \\ \cline{2-5} 
					& 100 & 88,63 & 68,07 & 1,392 \\ \hline
					30x30 & 500 & 87,08 & 70,17 & 1,491 \\ \cline{2-5} 
					& 400 & 86,56 & 72,27 & 1,472 \\ \cline{2-5} 
					& \textbf{300} & \textbf{88,63} & \textbf{73,11} & 1,435 \\ \cline{2-5} 
					& 200 & 88,37 & 72,26 & 1,409 \\ \cline{2-5} 
					& 100 & 89,15 & 67,65 & 1,388 \\ \hline
				\end{tabular}%
			}
		\end{table}
	\end{frame}
	
	\begin{frame}
		\frametitle{Siamese Neural Network: Introduction}
		As previously mentioned, age classification is a challenging problem due to the complexity of the features that make up a face.
		
		\bigskip
		
		
		So we chose a \textbf{discriminative} approach, since we want to be able to separate \textbf{children} from \textbf{non-children}.
		
		\bigskip
		
		
		This was achieved by taking advantage of a \textbf{Siamese neural network}\footnote{Actually there is only one network that is used to process the two inputs.} that takes two inputs: a \textbf{template} image and the input image from the face extractor and checks if they belong to the same class or not. 	
	\end{frame}
	
	\begin{frame}
		\frametitle{Siamese Neural Network - The Pairs}
		This kind of neural networks require in input a pair of images:
		\begin{itemize}
			\item Template image: the class example
			\item Input image: the image that has to be classified
		\end{itemize}		
		The label $(1,0)$ symbolizes that the template image and the input image belong to the same class, $(0,1)$ otherwise.
	\end{frame}
	
	\begin{frame}
		\frametitle{Siamese Neural Network - The Pairs}
		We selected 26 child images as templates and paired them with all the other images in the original dataset\footnote{We excluded the pairs which contained the same image}, obtaining a new larger set of samples. The same procedure has been done with the adults images.
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{img/siamese_training_set_children.JPG}
    		\caption{Siamese training - validation - test set (children network)}
    		\label{fig:siamese_general}
		\end{figure}
	\end{frame}
	
	\begin{frame}
		\frametitle{Siamese Neural Network - General Architecture}		
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{img/siamese_schema_generale.png}
    		\caption{General outline of the network, the CNN is the same for both images}
    		\label{fig:siamese_dataset}
		\end{figure}
	\end{frame}
	
	\begin{frame}
		\frametitle{Siamese Neural Network - Discrimination module}
		The CNN part of the system actually works as an image encoder, extracting features from both the template and the input image \footnote{Could be optimized at runtime by preprocessing the templates},
		which are then fed to the \textbf{discrimination module}.
		
		The paper\footfullcite{bromley1994signature} that inspired this approach used a joining neuron that calculated the cosine distance between the encoded vectors.
		
		We decided to implement two different discriminator modules, one based on the \textbf{euclidean distance} between the CNN-encoded vectors and for the other one a \textbf{multi-layered perceptron} which was fed the concatenation of the two encoded vectors.
	\end{frame}	
	
	\begin{frame}
		\frametitle{Siamese Neural Network - Model selection strategies}
		We began our work by implementing a modified, slimmed-down version of the VGG16 architecture, based on the remarkable results that this model obtained in \textbf{ILSVRC}\footnote{ImageNet Large Scale Visual Recognition Competition} 2014.
	
		\bigskip
		
		However, we were not satisfied with the results, so we decided to build a custom network and started the cross-validation process. This however was taking too long even for a small subset of hyperparameters (although it was giving very decent results when we stopped it, see table below).
		
		\bigskip
		
		So we devised a very simple evolutionary algorithm, with accuracy on validation set as fitness function.
	\end{frame}
	
	\begin{frame}
		\frametitle{Siamese Neural Network - Training results}
		We tested the resulting networks on the appropriately\footnote{Two different sets, one for adults and one for children} held-out test set and obtained\footnote{H = Hinge loss}
		\begin{table}[]
			\centering
			\resizebox{0.8\textwidth}{!}{%
				\begin{tabular}{||c c c||}
					\hline
					Model & Accuracy & Loss  \\ [0.5ex] 
					\hline\hline
					VGG16 + MLP (Adults) & 90.43\% & 0.5991 (H) \\ 
					\hline
					VGG16 + MLP (Child) & 90.24\% & 0.5986 (H) \\ 
					\hline
					CV (Child) & 93.06\% & 0.5751 (H) \\
					\hline
					\textbf{Evo (Child)} & \textbf{94.07\%} & \textbf{0.5628} (H)  \\
					\hline
					\textbf{Evo (Adults)} & \textbf{93.46\%} & \textbf{0.5658} (H) \\ [1ex] 
					\hline
				\end{tabular}%
			}
		\end{table}
	\end{frame}
	
	\begin{frame}
		\frametitle {Siamese Neural Network - Selected architectures}
		\begin{figure}
			\centering
			\includegraphics[width=0.9\textwidth, height=0.65\textheight]{img/schema_modelli_unico.png}
    		\caption{Selected architectures, Optimizer (both): Nadam}
    		\label{fig:selected_architectures}
		\end{figure}
	\end{frame}
	
	\section{Conclusions}
	
	\begin{frame}
		\frametitle{Improvements: Child-Seat Detection System}
		When used properly, only children should be sitting on the child-seat. This assumption suggests a completely different approach for the detection of the presence of the child in the car.
		
		\bigskip
		
		Instead of relying on the face detection and classification, we can assume that, when a child-seat is detected in a certain region, if a face is detected in the same region then it would belong to a child.
		
		\bigskip
		
		Under this assumption it would be possible to perform the face classification task fewer times in order to increase the general speed of the system.
	\end{frame}

	\begin{frame}
		\frametitle{Improvements: Child-Seat Detection System}
		We decided to employ SIFT descriptors for determining the region where the child-seat is. These descriptors represent features of the child-seat (owned by the user) which can be use to recognize future instances of it.
		
		\bigskip
		
		The collection of the SIFT descriptors would be performed through the same camera used for the detection. This ensures little variation in the point of view over the child-seat.
		
		\bigskip
		
		Note that the system must include a detector for a generic child-seat since the collection of the descriptors is \textit{bound} to the presence of the child-seat.
	\end{frame}

	\begin{frame}
		\frametitle{Improvements: Child-Seat Detection System}
		\begin{figure}
			\centering
			\includegraphics[width=0.7\textwidth]{img/SIFT_collection.png}
			\caption{Detection system block diagram - Overview.}
		\end{figure}
	\end{frame}

	\begin{frame}
		\frametitle{Improvements: Child-Seat Detection System}
		The steps for our generic child-seat detection system are:
		
		\begin{enumerate}
			\item Collection of SIFT descriptors from child-seat images;
			\item Creation of a Bag Of Visual Words $\mathcal{B}$ using K-Means clustering algorithm;
			\item Collection of SIFT descriptors from the camera input stream and representation in the BOVW feature space;
			\item We assume that the input $\hat{x}$ contains a child-seat if the following statement holds true for at least $n$ patterns in $\mathcal{B}$
		\end{enumerate}
		\begin{equation*}
			\norm{\hat{x} - x_{\mathcal{B}}} < \gamma,
			\quad x_{\mathcal{B}} \in \mathcal{B}.
		\end{equation*}
	\end{frame}

	\begin{frame}
		\frametitle{Improvements: Child-Seat Detection System}
		The following results were obtained by using:
		\begin{itemize}
			\item $197$ images taken from the internet;
			\item $50$ clusters when creating the BOVW;
			\item $n=5, \gamma=0.1$.
		\end{itemize}
		
		\bigskip
		
		\begin{minipage}{0.5\textwidth}
			\begin{center}
				{\small Input frames}
			\end{center}
		\end{minipage}%
		\begin{minipage}{0.5\textwidth}
			\begin{center}
				\begin{small}
					Frames where a child-seat\
					has been detected
				\end{small}
			\end{center}
		\end{minipage}
	
		\bigskip
		
		\includemedia[width=\textwidth,
					 addresource=videos/detection.mp4,
					 transparent,
					 activate=pagevisible,
					 flashvars={
					 	source=videos/detection.mp4
						&autoplay=true
						&loop=false}]
		{\textcolor{white}{Video}}{VPlayer9.swf}
	\end{frame}

	\begin{frame}
		\frametitle{Improvements: Child-Seat Detection System}
		From the frames where a generic child-seat is detected the SIFT descriptors are computed.
		
		\bigskip
		
		Ideally, if the car's backseat is free from clutter, the descriptors should gather around the child-seat. A good way to remove descriptors not belonging to the child-seat (i.e. the outliers) would be to use the mean and standard deviation of the key-points' position.
		
		\bigskip
		
		The detection of the user's child-seat is performed by comparing the SIFT descriptors taken in the previous step (i.e. the second block of the block diagram) with the new ones from the camera input.
	\end{frame}

	\begin{frame}
		\frametitle{Improvements: Child-Seat Detection System}
		Using two frames (and their horizontally flipped versions) from the unprocessed version of the video below, we obtained the following result.
		
		\bigskip
		
		\includemedia[width=\textwidth,
					 addresource=videos/BB.mp4,
					 transparent,
					 activate=pagevisible,
					 flashvars={
					 	source=videos/BB.mp4
					 	&autoplay=true
					 	&loop=false}]
		{\textcolor{white}{Video}}{VPlayer9.swf}
		
		The red circle is the mean of the key-points (black circles), the green bounding box includes all the key-points while the blue one is a $200 \times 200$ pixels square centered in the mean.
	\end{frame}

	\begin{frame}
		\frametitle{RGB vs BGR in OpenCV}
		TODO
	\end{frame}
	
	\begin{frame}
		\centering \Huge
		\emph{Thank You.}
	\end{frame}

\end{document}

